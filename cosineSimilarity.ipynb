{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "similarity.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alper-oner/checkworthy-claims/blob/master/cosineSimilarity.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-9sPQTmz2aS_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install flair"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uvy-fwfifu15",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install nltk\n",
        "import nltk\n",
        "import re\n",
        "import pandas as pd\n",
        "import io\n",
        "nltk.download('stopwords')\n",
        "WPT = nltk.WordPunctTokenizer()\n",
        "stop_word_list = nltk.corpus.stopwords.words('turkish')\n",
        "\n",
        "def norm_tweet(tweet):\n",
        "    tweet=tweet.replace(\"İ\",\"i\")\n",
        "    tweet=tweet.replace(\"Ö\",\"ö\")\n",
        "    tweet=tweet.replace(\"@\",\"\")\n",
        "    tweet=tweet.replace(\"'\", \"\")\n",
        "    tweet=tweet.replace(\"’\", \"\")\n",
        "    tweet=tweet.replace('\"', \"\")\n",
        "    tweet=tweet.replace(\"#\",\"\")\n",
        "    tweet=tweet.replace(\",\",\"\")\n",
        "    tweet=tweet.replace(\".\",\"\")\n",
        "    tweet=tweet.replace(\"!\",\"\")\n",
        "    tweet=tweet.replace(\";\",\"\")\n",
        "    tweet=tweet.replace(\":\",\"\")\n",
        "    tweet=tweet.replace(\"/\",\"\")\n",
        "    tweet=tweet.replace(\"“\",\"\")\n",
        "    tweet=tweet.replace(\"”\",\"\")\n",
        "    tweet=tweet.replace(\"?\",\"\")\n",
        "    tweet=tweet.replace(\"-\",\"\")\n",
        "\n",
        "    pattern = r\"[{}]\".format(\",.;@#!\") \n",
        "    tweet = re.sub(pattern, \"\", str(tweet))\n",
        "    tweet = tweet.lower()\n",
        "    tweet = tweet.strip()\n",
        "    tokens = WPT.tokenize(tweet)\n",
        "    filtered_tokens = [token for token in tokens if token not in stop_word_list]\n",
        "    tweet = ' '.join(filtered_tokens)\n",
        "    return tweet"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hWmYnjut0lw4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import urllib.request, json \n",
        "import ssl\n",
        "\n",
        "\n",
        "context = ssl._create_unverified_context()\n",
        "\n",
        "with urllib.request.urlopen(\"https://api.myjson.com/bins/1bgkqj\",context=context) as url:\n",
        "    dataClaim = json.loads(url.read().decode())\n",
        "\n",
        "claimList=[]\n",
        "for claim in dataClaim[\"claims\"]:\n",
        "    #print(claim[\"text\"])\n",
        "    claimList.append(claim[\"text\"])\n",
        "\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xKQp548Z4Qrx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import urllib.request, json \n",
        "import ssl\n",
        "\n",
        "context = ssl._create_unverified_context()\n",
        "\n",
        "# test_balanced : https://api.myjson.com/bins/qnijh\n",
        "# test_unbalanced : https://api.myjson.com/bins/7da99\n",
        "with urllib.request.urlopen(\"https://api.myjson.com/bins/7da99\",context=context) as url:\n",
        "    dataTweet = json.loads(url.read().decode())\n",
        "\n",
        "tweetList=[]\n",
        "for tweet in dataTweet:\n",
        "    #print(tweet[\"tweet\"])\n",
        "    tweetList.append(tweet[\"tweet\"])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jGXkVFBzExtX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from flair.embeddings import WordEmbeddings, FlairEmbeddings, DocumentPoolEmbeddings, Sentence, CharacterEmbeddings\n",
        "\n",
        "# initialize the word embeddings\n",
        "tr_embedding = WordEmbeddings('tr')\n",
        "char_embedding = CharacterEmbeddings()\n",
        "\n",
        "# initialize the document embeddings, mode = mean\n",
        "document_embeddings = DocumentPoolEmbeddings([tr_embedding,char_embedding])\n",
        "claimTensors=[]\n",
        "\n",
        "for claim in claimList:\n",
        "    #print(norm_tweet(claim))\n",
        "    sentence = Sentence(norm_tweet(claim))\n",
        "    document_embeddings.embed(sentence)\n",
        "    claimTensors.append(sentence.get_embedding().data)  \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LZ2B7gPO7hHC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from flair.embeddings import WordEmbeddings, FlairEmbeddings, DocumentPoolEmbeddings, Sentence, CharacterEmbeddings\n",
        "\n",
        "tweetTensors=[]\n",
        "for tweet in tweetList:\n",
        "    #print(norm_tweet(tweet))\n",
        "    sentence = Sentence(norm_tweet(tweet))\n",
        "    document_embeddings.embed(sentence)\n",
        "    tweetTensors.append(sentence.get_embedding().data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TTWH_8W49nJE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from scipy import spatial\n",
        "check_worthy_tweet=[]\n",
        "for tweeetIndex in range(len(tweetTensors)):\n",
        "  for claimIndex in range(len(claimTensors)):\n",
        "    result = 1 - spatial.distance.cosine(tweetTensors[tweeetIndex],claimTensors[claimIndex])\n",
        "    if result > 0.80:\n",
        "      if tweetList[tweeetIndex] not in check_worthy_tweet:\n",
        "        check_worthy_tweet.append(tweetList[tweeetIndex])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LxOCDWSmCJzS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import confusion_matrix \n",
        "from sklearn.metrics import accuracy_score \n",
        "from sklearn.metrics import classification_report \n",
        "\n",
        "\n",
        "actual = []\n",
        "predicted = []\n",
        "\n",
        "for tweet in dataTweet:\n",
        "    if tweet[\"has_checkworthy_claim\"] == 1:\n",
        "      actual.append(1)\n",
        "    else:\n",
        "      actual.append(0)\n",
        "      \n",
        "for tweet in dataTweet:\n",
        "    if tweet[\"tweet\"] in check_worthy_tweet:\n",
        "      predicted.append(1)\n",
        "    else:\n",
        "      predicted.append(0)\n",
        "\n",
        "\n",
        "results = confusion_matrix(actual, predicted) \n",
        "print ('Confusion Matrix :')\n",
        "print(results) \n",
        "print ('Accuracy Score :',accuracy_score(actual, predicted) )\n",
        "print ('Report : ')\n",
        "print (classification_report(actual, predicted))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tke63dxdBlcY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install pandas_ml\n",
        "from pandas_ml import ConfusionMatrix\n",
        "cm = ConfusionMatrix(actual, predicted)\n",
        "cm.print_stats()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}